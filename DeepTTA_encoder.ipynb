{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "794f0256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0721,  0.0248,  0.1143,  ..., -0.0746,  0.0595,  0.0000],\n",
      "        [ 0.0828,  0.1275, -0.1284,  ...,  0.1092,  0.0576,  0.0800],\n",
      "        [-0.0469, -0.0000, -0.0524,  ..., -0.0528, -0.0266,  0.0613],\n",
      "        ...,\n",
      "        [ 0.1255, -0.0769, -0.1521,  ..., -0.1169, -0.0284,  0.0841],\n",
      "        [ 0.1447,  0.0871,  0.1154,  ..., -0.0131, -0.1329, -0.0494],\n",
      "        [-0.1311,  0.0324, -0.0182,  ..., -0.1411,  0.1170, -0.0777]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Smiles_To_Emmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        #tokenization:\n",
    "        vocab_file=open('drug_codes_chembl_freq_1500.txt','r')\n",
    "        vocab_data=vocab_file.read()\n",
    "        vocab_data=vocab_data.replace(' ','')\n",
    "        self.vocab=vocab_data.split('\\n')\n",
    "        self.vocab.pop(0)\n",
    "        self.max_lenght= len(max(vocab, key=len))\n",
    "        self.zeta=50 #max lengt of drug representation\n",
    "        \n",
    "        SMILES_CHARS = [' ',\n",
    "                '#', '%', '(', ')', '+', '-', '.', '/',\n",
    "                '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                '=', '@',\n",
    "                'A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P',\n",
    "                'R', 'S', 'T', 'V', 'X', 'Z',\n",
    "                '[', '\\\\', ']',\n",
    "                'a', 'b', 'c', 'e', 'g', 'i', 'l', 'n', 'o', 'p', 'r', 's',\n",
    "                't', 'u']\n",
    "        self.vocab=self.vocab+SMILES_CHARS\n",
    "        \n",
    "        self.smi2index = dict( (c,i) for i,c in enumerate( vocab ) )\n",
    "        self.index2smi = dict( (i,c) for i,c in enumerate( vocab ) )\n",
    "        \n",
    "        #encoder layer:\n",
    "        self.gamma=50\n",
    "        dropout_rate=0.05\n",
    "        self.chem_embedding=torch.nn.Linear(len(self.vocab),self.gamma,bias=False)\n",
    "        self.pos_embedding=torch.nn.Linear(self.zeta,self.gamma,bias=False)      \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        \n",
    "    def smiles_encoder(self,smiles ):\n",
    "        X = np.zeros( (len(self.vocab), self.zeta ) )\n",
    "        n_lower=0\n",
    "        n_upper=self.max_lenght\n",
    "        j=0\n",
    "        while(n_lower<n_upper):\n",
    "            try:\n",
    "                i=self.smi2index[smiles[n_lower:n_upper]]\n",
    "                #print(smiles[n_lower:n_upper],'found, position',(i,j))\n",
    "                X[i,j] = 1\n",
    "                n_lower=n_upper\n",
    "                n_upper=min(n_upper+max_lenght,len(smiles)+1)\n",
    "                j+=1\n",
    "            except: \n",
    "                n_upper-=1\n",
    "                #print('fail, new',smiles[n_lower:n_upper])\n",
    "        return X\n",
    "\n",
    "    def smiles_decoder(self,X ):\n",
    "        smi = ''\n",
    "        X = X.argmax( axis=0 )\n",
    "        for i in X:\n",
    "            if(i==0): break\n",
    "            smi += index2smi[ i ]\n",
    "        return smi\n",
    "    \n",
    "    def forward(self,smiles):\n",
    "        M=torch.from_numpy(self.smiles_encoder(smiles)).float()\n",
    "        C=torch.zeros([self.gamma,self.zeta])\n",
    "        P=torch.zeros([self.gamma,self.zeta])\n",
    "        for j in range(self.zeta):\n",
    "            C[:,j]=self.chem_embedding(M[:,j])\n",
    "            I=torch.zeros(self.zeta)\n",
    "            I[j]=1\n",
    "            P[:,j]=self.pos_embedding(I) \n",
    "        E=C+P\n",
    "        E=self.dropout(E)\n",
    "        return (E)\n",
    "    \n",
    "Encoder=Smiles_To_Emmbedding()\n",
    "X=Encoder.forward('C1CC2=C(C=C(C=C2)C3=NC(=C(S3)CCCOC4=CC=C(C=C4)CN)C(=O)O)/C(=N/NC5=NC6=CC=CC=C6S5)/C1')\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd5e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
